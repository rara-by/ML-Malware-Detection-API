import json
import boto3
import pefile
from scipy.sparse import hstack, csr_matrix
from joblib import load
import collections
from nltk import ngrams
import numpy as np
import argparse

parser = argparse.ArgumentParser(description='Process sample input for prediction.')
parser.add_argument('--sample', type=str, help='Path to the sample input file')
args = parser.parse_args()

# Initialize the SageMaker runtime
runtime = boto3.client('sagemaker-runtime')

N = 2
K1_most_common_Ngrams_list = [
     (0, 0), (255, 255), (204, 204), (2, 100), (1, 0), (0, 139), (131, 196), (2, 0), (68, 36), (139, 69), (0, 131), (46, 46),
     (255, 117), (133, 192), (255, 139), (254, 255), (141, 77), (139, 77), (255, 21), (69, 252), (76, 36), (0, 1), (8, 139), (7, 0), (4, 0),
     (137, 69), (4, 139), (255, 131), (141, 69), (0, 137), (51, 192), (0, 255), (80, 232), (255, 141), (3, 100), (85, 139), (8, 0),
     (0, 116), (0, 232), (15, 182), (100, 0), (80, 141), (15, 132), (139, 236), (255, 0), (65, 68), (73, 78), (84, 36), (12, 139), (68, 68),
     (80, 65), (78, 71), (68, 73), (64, 0), (253, 255), (16, 0), (198, 69), (0, 204), (199, 69), (192, 116), (3, 0), (139, 68), (80, 255),
     (4, 137), (116, 36), (101, 0), (204, 139), (100, 139), (139, 76), (64, 2), (106, 0), (196, 12), (0, 8), (36, 8), (196, 4), (32, 0),
     (139, 70), (0, 89), (117, 8), (100, 232), (69, 8), (95, 94), (0, 16), (86, 139), (131, 192), (0, 80), (0, 117), (36, 20),
     (0, 141), (195, 204), (1, 100), (139, 240), (36, 16), (0, 128), (2, 101), (9, 0), (139, 255), (6, 0), (131, 248), (0, 32)]

# Define functions for feature extraction
def readFile(filePath):
    with open(filePath, "rb") as binary_file:
        data = binary_file.read()
    return data

def byteSequenceToNgrams(byteSequence, n):
    Ngrams = ngrams(byteSequence, n)
    return list(Ngrams)
    
def extractNgramCounts(file, N):
    fileByteSequence = readFile(file)
    fileNgrams = byteSequenceToNgrams(fileByteSequence, N)
    return collections.Counter(fileNgrams)

def getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list):
    K1 = len(K1_most_common_Ngrams_list)
    fv = K1*[0]
    fileNgrams = extractNgramCounts(file, N)
    for i in range(K1):
        fv[i]=fileNgrams[K1_most_common_Ngrams_list[i]]
    return fv

def preprocessImports(listOfDLLs):
    processedListOfDLLs = []
    temp = [x.decode().split(".")[0].lower() for x in listOfDLLs]
    return " ".join(temp)

def getImports(pe):
    listOfImports = []
    for entry in pe.DIRECTORY_ENTRY_IMPORT:
        listOfImports.append(entry.dll)
    return preprocessImports(listOfImports)

def getSectionNames(pe):
    listOfSectionNames = []
    for eachSection in pe.sections:
        refined_name = eachSection.Name.decode().replace('\x00','').lower()
        listOfSectionNames.append(refined_name)
    return " ".join(listOfSectionNames)

def extract_features(sample_test, imports_featurizer, section_names_featurizer):
    importsCorpus_test = []
    numSections_test = []
    sectionNames_test = []
    NgramFeaturesList_test = []
    labels = []
    
    # check if sample_test is list
    if not isinstance(sample_test, list):
        sample_test = [sample_test] #path to data 

    for file in sample_test:
        NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)
        pe = pefile.PE(file)
        imports = getImports(pe)
        nSections = len(pe.sections)
        secNames = getSectionNames(pe)
        importsCorpus_test.append(imports)
        numSections_test.append(nSections)
        sectionNames_test.append(secNames)
        NgramFeaturesList_test.append(NGramFeatures)

        label = file.split('/')[-2]  
        labels.append(label)

        # Transform features using pre-trained featurizers
        importsCorpus_test_transformed = imports_featurizer.transform(importsCorpus_test)
        sectionNames_test_transformed = section_names_featurizer.transform(sectionNames_test)

        # Combine features into a single feature vector
        X_test = hstack([NgramFeaturesList_test, importsCorpus_test_transformed, sectionNames_test_transformed, 
                           csr_matrix(numSections_test).transpose()])

        return X_test, labels

#         except Exception as e: 
#             print(file + ":")
#             print(e)
        
    

# Load pre-trained featurizers
imports_featurizer = load('imports_featurizer.joblib')
section_names_featurizer = load('section_names_featurizer.joblib')


# Your sample test data
if args.sample:
    sample_test = args.sample
else:
    sample_test = ['Training a Static Malware Detector/Code/Samples/Benign/file-uri.exe']  # default sample benign

# Extract features for the test sample
X_test, labels = extract_features(sample_test, imports_featurizer, section_names_featurizer)

# Prepare endpoint JSON payload
data = X_test.data.tolist()
row = X_test.row.tolist()
col = X_test.col.tolist()
shape = X_test.shape


payload = {
    "data": data,
    "row": row,
    "col": col,
    "shape": shape
}

json_payload = json.dumps(payload)


# Specify the endpoint name
endpoint_name = "sagemaker-scikit-learn-2024-03-29-03-21-47-783"

# Invoke the endpoint
response = runtime.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType='application/json',
    Body=json.dumps(json_payload)
)

# Parse the response
result = json.loads(response['Body'].read().decode())
print("Prediction result:", result)
print("True label: ", labels)